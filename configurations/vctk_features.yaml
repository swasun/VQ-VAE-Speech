decoder_type: 'deconvolutional'

# Training
batch_size: 1 # batch_size per a GPU
data_root: './' # ./ == vctk/raw/VCTK-Corpus
start_epoch: 0
num_epochs: 5
num_workers: 1
train_val_split: 0.8
learning_rate: 0.0002

# Cuda
use_cuda: True
use_data_parallel: False
use_device: 

# Audio
sampling_rate: 16000 # Sampling rate
res_type: 'kaiser_fast' # Resampling algorithm
top_db: 20 # The threshold (in decibels) below reference to consider as silence
length: 7680

# Mu-law
quantize: 256

# Encoder
num_hiddens: 768
input_dim: 256

# VQ
num_embeddings: 29 # The higher this value, the higher the capacity in the information bottleneck.

# This value is not that important, usually 64 works.
# This will not change the capacity in the information-bottleneck.
embedding_dim: 64

# Commitment cost should be set appropriately. It's often useful to try a couple
# of values. It mostly depends on the scale of the reconstruction cost
# (log p(x|z)). So if the reconstruction cost is 100x higher, the
# commitment_cost should also be multiplied with the same amount.
# 0.25 as specified in the paper.
commitment_cost: 0.25

# Only uses for the EMA updates (instead of the Adam optimizer).
# This typically converges faster, and makes the model less dependent on choice
# of the optimizer. In the original VQ-VAE paper [van den Oord et al., 2017],
# EMA updates were not used (but suggested in appendix) and compared in
# [Roy et al., 2018].
decay: 0.0

# Residual
residual_channels: 768
num_residual_layers: 2

# Features
input_features_type: 'mfcc'
output_features_type: 'logfbank'
input_features_dim: 47
input_features_filters: 13
output_features_dim: 47
output_features_filters: 80
augment_input_features: True
augment_output_features: False

# Conv

# Weight initialization proposed by [He, K et al., 2015].
# PyTorch doc: https://pytorch.org/docs/stable/nn.html#torch.nn.init.kaiming_normal_.
# The model seems to converge faster using it.
# In addition to that, I used nn.utils.weight_norm() before each use of kaiming_normal(),
# as they do in [ksw0306/ClariNet], because it works better.
use_kaiming_normal: False

# jitter layer
jitter_probability: 0.12
use_jitter: False
